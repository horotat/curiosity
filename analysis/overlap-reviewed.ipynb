{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntokens: 4238\n"
     ]
    }
   ],
   "source": [
    "import os,sys,inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# savings\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import agent\n",
    "\n",
    "from toolbox import get_word_ix, UniversalConstants, UniversalData, model_best_acc_detector, load_listener_model, load_speaker_model, curiosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required for the functions defined here. They should be imported to toolbox when the function definitions move there.\n",
    "import numpy as np\n",
    "from toolbox import load_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc = UniversalConstants()\n",
    "ud = UniversalData()\n",
    "\n",
    "setting = 'curious'\n",
    "which_set = 'val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlapping Analysis\n",
    "\n",
    "This notebook intends to do the steps below:\n",
    "\n",
    "1. [Choose the most accurate random model](#best_random)\n",
    "2. [Extract the choices on each epoch](#data_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Choose the most accurate `random` model <a name=\"best_random\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the loss_acc results to choose the best model\n",
    "path_loss_acc = os.path.join('/home', 'u1270964', 'curiosity', 'loss_acc', 'unsupervised/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This definition should move to toolbox\n",
    "def best_model_acc(_path_loss_acc, _agent, _setting, which_set, learning, mode='single'):\n",
    "    if _agent == 'listener':\n",
    "            _agent = 'li'\n",
    "    elif _agent == 'speaker':\n",
    "            _agent = 'sp'\n",
    "    else:\n",
    "        print(\"put either listener or speaker for agent.\")\n",
    "    \n",
    "    if mode == 'single':\n",
    "        \n",
    "        _the_best = {'best_seed': 0,\n",
    "                     'best_ep': 0,\n",
    "                     'best_acc': 0.0}\n",
    "\n",
    "        for _seed in uc.final_seeds:\n",
    "            scores = load_numpy(_path_loss_acc, _agent, which_set, 'acc', 0.001, _setting, _seed)\n",
    "            if max(scores) > _the_best['best_acc']:\n",
    "                _the_best['best_seed'] = _seed\n",
    "                _the_best['best_ep'] = scores.argmax() + 1\n",
    "                _the_best['best_acc'] = scores[_the_best['best_ep'] - 1]\n",
    "    \n",
    "    elif mode == 'avg':\n",
    "        _the_best = {'best_seed': 0,\n",
    "                     'best_ep': 0,\n",
    "                     'best_avg_acc': 0.0,\n",
    "                     'top_acc': 0.0,\n",
    "                     'min_acc': 0.0,\n",
    "                     'scores': []}\n",
    "\n",
    "        for _seed in uc.final_seeds:\n",
    "            \n",
    "            scores = load_numpy(_path_loss_acc, _agent, which_set, 'acc', 0.001, _setting, _seed)\n",
    "            acc_avg = np.mean(scores)\n",
    "            # acc_avg = np.average(scores, weights=np.arange(1,41))\n",
    "            if acc_avg > _the_best['best_avg_acc']:\n",
    "                _the_best['best_seed'] = _seed\n",
    "                _the_best['best_ep'] = scores.argmax() + 1\n",
    "                _the_best['best_avg_acc'] = acc_avg\n",
    "                _the_best['top_acc'] = max(scores)\n",
    "                _the_best['min_acc'] = min(scores)\n",
    "                _the_best['scores'] = scores\n",
    "    else:\n",
    "        print('mode should be either \\'single\\' or \\'avg\\'.')\n",
    "\n",
    "    return _the_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------\n",
      "The most accurate random model is:\n",
      "\n",
      "Best Seed:\t\t 890 \n",
      "Avearge Accuracy:\t 0.66 \n",
      "---\n",
      "Lowest Accuracy:\t 0.52 \n",
      "Highest Accuracy:\t 0.69 \n",
      "----------------------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7cd9a1f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_random_model = best_model_acc(path_loss_acc, 'listener', setting, which_set, uc.final_seeds, 'avg')\n",
    "\n",
    "print('\\n'+\"-\"*35+\"\\nThe most accurate random model is:\\n\\n\"\n",
    "      +\"Best Seed:\\t\\t\", best_random_model['best_seed'],\n",
    "      \"\\nAvearge Accuracy:\\t\", format(best_random_model['best_avg_acc'], '.2f'),\n",
    "      \"\\n---\\nLowest Accuracy:\\t\", format(best_random_model['min_acc'], '.2f'),\n",
    "      \"\\nHighest Accuracy:\\t\", format(best_random_model['top_acc'], '.2f'),\n",
    "      '\\n'+\"-\"*35,'\\n')\n",
    "\n",
    "figure = plt.figure(figsize=[8,4])\n",
    "plt.plot(np.arange(1,41), best_random_model['scores'],\n",
    "         label='Best Average Random Model', color=\"red\", linewidth=3)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.ylabel('Accuracy', fontsize=13)\n",
    "plt.xlabel('Epoch', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(a=best_random_model['best_seed'])\n",
    "\n",
    "# setting torch seeds\n",
    "torch.cuda.manual_seed(best_random_model['best_seed'])\n",
    "torch.manual_seed(best_random_model['best_seed'])\n",
    "\n",
    "np.random.seed(best_random_model['best_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract the choices of different curiosity metrics <a name\"data_extraction\"></a>\n",
    "\n",
    "I will load the same model for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_1. for each epoch:_\n",
    ">1.1. load the model[epoch]\n",
    "\n",
    ">1.2. for all img in val_split:\n",
    ">>1.2.1. for each condition:\n",
    ">>>1.2.1.1. record the choices\n",
    "\n",
    "Should check:\n",
    "    \n",
    "- difference of train batch set and val batch set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(_dict_words_boxes, _ha_vggs_indices, _word_to_ix, img, _device, path_vgg):\n",
    "    vggs = torch.load(path_vgg + img + \".pt\").to(_device)  # Edit path\n",
    "    # dict met obj ids als keys en een dictionary met words : '', bboxes :\n",
    "    # n = 0\n",
    "    bbox_indices = []\n",
    "    words = []\n",
    "    for obj in _dict_words_boxes[img]:  # For every object in this image\n",
    "        words.append(get_word_ix(_word_to_ix, _dict_words_boxes[img][obj][\"word\"]))\n",
    "        bbox_indices.append(_ha_vggs_indices[img][obj][0])\n",
    "    visual_input = vggs[bbox_indices, :]\n",
    "    language_input = torch.tensor(words, dtype=torch.long, device=_device)\n",
    "    return language_input, visual_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curious_look(_dict_words_boxes, _ha_vggs_indices, _img, _setting, _word_to_ix, _device):\n",
    "    language_input, scene = load_img(_dict_words_boxes, _ha_vggs_indices, _word_to_ix, _img, _device, uc.path_vgg)\n",
    "    # repeat scene n_objects times as input to listener\n",
    "    visual_input = scene.expand(scene.size()[0], scene.size()[0], scene.size()[1])\n",
    "    curiosity_targets = torch.eye(visual_input.size()[0], dtype=torch.float, device=ud.device)\n",
    "    # targets is simply 0, 1, ...., n because they are in order of appearance\n",
    "    targets = torch.tensor([i for i in range(len(language_input))], dtype=torch.long, device=ud.device)\n",
    "    # word guesses by child - use as attention over word embeddings\n",
    "    word_guesses = speaker(visual_input, curiosity_targets, apply_softmax=False)\n",
    "    # only keep most likely words\n",
    "    words = torch.argmax(word_guesses, dim=1)\n",
    "    # give these as input to listener\n",
    "    object_guesses = listener(words, visual_input)\n",
    "    curiosity_values = curiosity(curiosity_targets, object_guesses, _setting)\n",
    "    max_curious = torch.argmax(curiosity_values)\n",
    "    return max_curious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_count(list_1, list_2):\n",
    "    pair_check = []\n",
    "    \n",
    "    if len(list_1)==len(list_2):\n",
    "        for _index in range(len(list_1)):\n",
    "            if list_1[_index] == list_2[_index]:\n",
    "                pair_check.append(1)\n",
    "            else:\n",
    "                pair_check.append(0)\n",
    "    else:\n",
    "        print(\"THE LISTS SHOULD HAVE THE SAME LENGTH!\")\n",
    "    \n",
    "    pair_count = pair_check.count(1)\n",
    "    \n",
    "    return {'count': pair_count, 'vec': pair_check}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trip_count(list_1, list_2, list_3):\n",
    "    trip_check = []\n",
    "    \n",
    "    if (len(list_1)==len(list_2)) and (len(list_2)==len(list_3)):\n",
    "        for _index in range(len(list_1)):\n",
    "            if (list_1[_index] == list_2[_index]) and (list_2[_index] == list_3[_index]):\n",
    "                trip_check.append(1)\n",
    "            else:\n",
    "                trip_check.append(0)\n",
    "    else:\n",
    "        print(\"THE LISTS SHOULD HAVE THE SAME LENGTH!\")\n",
    "    \n",
    "    trip_count = trip_check.count(1)\n",
    "    \n",
    "    return {'count': trip_count, 'vec': trip_check}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 2, 'vec': [0, 1, 0, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_count([1,2,3,4], [4,2,3,4], [1,2,6,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_listener_models(path_models_folder, _word_to_ix, _device, setting: str = 'curious', epoch: int = 0, lr: float = 0.001,\n",
    "                        seed: int = 123) -> agent.Listener:\n",
    "    \"\"\"\n",
    "    Loads the listener model\n",
    "    -----\n",
    "\n",
    "    :param _word_to_ix:\n",
    "    :type _word_to_ix:\n",
    "    :param _device:\n",
    "    :type _device:\n",
    "    :param path_models_folder:\n",
    "    :type path_models_folder:\n",
    "    :param setting:\n",
    "    :type setting:\n",
    "    :param epoch:\n",
    "    :type epoch:\n",
    "    :param lr:\n",
    "    :type lr:\n",
    "    :param seed:\n",
    "    :type seed:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    path_wanted_model = os.path.join(path_models_folder, setting,\n",
    "                                     'liModel_{}_{}_{}_ep{}.pth'.format(setting, lr, seed, epoch))\n",
    "\n",
    "    uc = UniversalConstants()\n",
    "\n",
    "    ntokens = len(_word_to_ix.keys())\n",
    "\n",
    "    # initializing the model\n",
    "    model = agent.Listener(uc.object_size, ntokens, uc.wordemb_size,\n",
    "                           uc.att_hidden_size, nonlinearity=uc.nonlin).to(_device)\n",
    "    # load state_dict to it\n",
    "    checkpoint = torch.load(path_wanted_model)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(\"\\nModel's state_dict:\")  # print the loaded state_dict\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    print((('-' * 60) + '\\n') * 2)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_speaker_models(path_models_folder, _word_to_ix, _device, setting: str = 'curious', epoch: int = 0, lr: float = 0.001,\n",
    "                        seed: int = 123) -> agent.Speaker:\n",
    "    \"\"\"\n",
    "    Loads the Speaker model\n",
    "    -----\n",
    "\n",
    "    :param _word_to_ix:\n",
    "    :type _word_to_ix:\n",
    "    :param _device:\n",
    "    :type _device:\n",
    "    :param path_models_folder:\n",
    "    :type path_models_folder:\n",
    "    :param setting:\n",
    "    :type setting:\n",
    "    :param epoch:\n",
    "    :type epoch:\n",
    "    :param lr:\n",
    "    :type lr:\n",
    "    :param seed:\n",
    "    :type seed:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    path_wanted_model = os.path.join(path_models_folder, setting,\n",
    "                                     'spModel_{}_{}_{}_ep{}.pth'.format(setting, lr, seed, epoch))\n",
    "\n",
    "    uc = UniversalConstants()\n",
    "    ntokens = len(_word_to_ix.keys())\n",
    "\n",
    "    # initializing the model\n",
    "    model = agent.Speaker(uc.object_size, ntokens, uc.att_hidden_size, nonlinearity=uc.nonlin).to(_device)\n",
    "    # load state_dict to it\n",
    "    checkpoint = torch.load(path_wanted_model)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(\"\\nModel's state_dict:\")  # print the loaded state_dict\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    print((('-' * 60) + '\\n') * 2)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************\n",
      "|EPOCH:\t 0 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 1 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 2 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 3 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 4 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 5 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 6 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 7 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 8 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************\n",
      "|EPOCH:\t 9 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 10 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 11 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 12 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 13 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 14 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 15 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 16 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 17 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************\n",
      "|EPOCH:\t 18 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 19 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 20 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 21 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 22 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 23 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 24 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 25 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 26 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************\n",
      "|EPOCH:\t 27 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 28 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 29 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 30 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 31 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 32 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 33 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 34 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 35 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************\n",
      "|EPOCH:\t 36 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 37 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 38 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 39 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "*************\n",
      "|EPOCH:\t 40 |\n",
      "*************\n",
      "\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 123 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 234 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 345 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 456 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 567 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 678 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 789 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 890 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 901 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 12 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 23 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 34 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 45 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 56 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 67 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 78 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 89 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 90 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 1 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- - - - - - - -\n",
      "|SEED:\t 100 |\n",
      "- - - - - - - -\n",
      "\n",
      "\n",
      "Initializing word embeddings\n",
      "Initializing attention MLP weights...\n",
      "Initializing bias terms to all 0...\n",
      "att_hidden.bias\n",
      "attention.bias\n",
      "\n",
      "Model's state_dict:\n",
      "word_embedder.weight \t torch.Size([4238, 256])\n",
      "att_hidden.weight \t torch.Size([256, 4352])\n",
      "att_hidden.bias \t torch.Size([256])\n",
      "attention.weight \t torch.Size([1, 256])\n",
      "attention.bias \t torch.Size([1])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "Initializing mapping weights...\n",
      "Initializing bias terms to all 0\n",
      "hidden.bias\n",
      "word_logits.bias\n",
      "\n",
      "Model's state_dict:\n",
      "hidden.weight \t torch.Size([256, 4096])\n",
      "hidden.bias \t torch.Size([256])\n",
      "word_logits.weight \t torch.Size([4238, 256])\n",
      "word_logits.bias \t torch.Size([4238])\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      " -^*-^*-^*-^*-^*-^*-^*-^*-^*-^* \n",
      " Total running time:\t 2691.79 \n",
      " -^*-^*-^*-^*-^*-^*-^*-^*-^*-^* \n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "y = {}\n",
    "for _seed in uc.final_seeds:\n",
    "    y[str(_seed)] = {\n",
    "        'pl_sn': [],\n",
    "        'pl_cu': [],\n",
    "        'cu_sn': [],\n",
    "        'all': []\n",
    "    }\n",
    "    \n",
    "averaging = {\n",
    "        'pl_sn': [],\n",
    "        'pl_cu': [],\n",
    "        'cu_sn': [],\n",
    "        'all': []\n",
    "        }\n",
    "\n",
    "epochs = np.arange(0,41)\n",
    "\n",
    "curious_choices = {}\n",
    "for _seed in uc.final_seeds:\n",
    "    curious_choices[str(_seed)] = {}\n",
    "    for epoch in [\"ep\"+str(ep) for ep in epochs]:\n",
    "        curious_choices[str(_seed)][epoch] = {\n",
    "            'single': {\n",
    "                'pl': [],\n",
    "                'sn': [],\n",
    "                'cu': []\n",
    "            },\n",
    "            'pair': {\n",
    "                'pl_sn': {'vec': [], 'match': 0},\n",
    "                'pl_cu': {'vec': [], 'match': 0},\n",
    "                'cu_sn': {'vec': [], 'match': 0},\n",
    "                'all': {'vec': [], 'match': 0}\n",
    "            }\n",
    "        \n",
    "        }\n",
    "\n",
    "for epoch in epochs:\n",
    "    \n",
    "    print(\"\\n*************\\n|EPOCH:\\t\", epoch, \"|\\n*************\\n\\n\")\n",
    "    # load all the models of the specified setting with all the seeds.\n",
    "    suming = {\n",
    "        'pl_sn': 0,\n",
    "        'pl_cu': 0,\n",
    "        'cu_sn': 0,\n",
    "        'all': 0\n",
    "        }\n",
    "    for _seed in uc.final_seeds:\n",
    "        print(\"\\n- - - - - - - -\\n|SEED:\\t\", _seed, \"|\\n- - - - - - - -\\n\\n\")\n",
    "        listener = load_listener_models(\"/home/u1270964/curiosity/unsupervised/models/\", ud.indexed_vocab, uc.device, setting, epoch, 0.001, _seed)\n",
    "        speaker = load_speaker_models(\"/home/u1270964/curiosity/unsupervised/models/\", ud.indexed_vocab, uc.device, setting, epoch, 0.001, _seed)\n",
    "        listener.eval()\n",
    "        speaker.eval()\n",
    "    \n",
    "        # batch = 0\n",
    "        n_batches = len(ud.val_batchlist)\n",
    "    \n",
    "        for batch_n in range(n_batches):\n",
    "            for img in ud.val_batchlist[batch_n]:\n",
    "                curious_choices[str(_seed)][\"ep\"+str(epoch)]['single']['pl'].append(curious_look(ud.dict_words_boxes, ud.ha_vggs_indices, img, 'plasticity', ud.indexed_vocab, uc.device).item())\n",
    "                curious_choices[str(_seed)][\"ep\"+str(epoch)]['single']['sn'].append(curious_look(ud.dict_words_boxes, ud.ha_vggs_indices, img, 'sn', ud.indexed_vocab, uc.device).item())\n",
    "                curious_choices[str(_seed)][\"ep\"+str(epoch)]['single']['cu'].append(curious_look(ud.dict_words_boxes, ud.ha_vggs_indices, img, 'curious', ud.indexed_vocab, uc.device).item())\n",
    "    \n",
    "        curious_choices[str(_seed)][\"ep\"+str(epoch)]['pair']['pl_sn'] = pair_count(curious_choices[str(_seed)][\"ep\"+str(epoch)]['single']['pl'], curious_choices[str(_seed)][\"ep\"+str(epoch)]['single']['sn'])\n",
    "        curious_choices[str(_seed)][\"ep\"+str(epoch)]['pair']['pl_cu'] = pair_count(curious_choices[str(_seed)][\"ep\"+str(epoch)]['single']['pl'], curious_choices[str(_seed)][\"ep\"+str(epoch)]['single']['cu'])\n",
    "        curious_choices[str(_seed)][\"ep\"+str(epoch)]['pair']['cu_sn'] = pair_count(curious_choices[str(_seed)][\"ep\"+str(epoch)]['single']['cu'], curious_choices[str(_seed)][\"ep\"+str(epoch)]['single']['sn'])\n",
    "        curious_choices[str(_seed)][\"ep\"+str(epoch)]['pair']['all'] = trip_count(curious_choices[str(_seed)][\"ep\"+str(epoch)]['single']['pl'], curious_choices[str(_seed)][\"ep\"+str(epoch)]['single']['sn'], curious_choices[str(_seed)][\"ep\"+str(epoch)]['single']['cu'])\n",
    "        \n",
    "        y[str(_seed)]['pl_sn'].append(curious_choices[str(_seed)][\"ep\"+str(epoch)]['pair']['pl_sn']['count'])\n",
    "        suming['pl_sn'] += curious_choices[str(_seed)][\"ep\"+str(epoch)]['pair']['pl_sn']['count']\n",
    "        \n",
    "        y[str(_seed)]['pl_cu'].append(curious_choices[str(_seed)][\"ep\"+str(epoch)]['pair']['pl_cu']['count'])\n",
    "        suming['pl_cu'] += curious_choices[str(_seed)][\"ep\"+str(epoch)]['pair']['pl_cu']['count']\n",
    "        \n",
    "        y[str(_seed)]['cu_sn'].append(curious_choices[str(_seed)][\"ep\"+str(epoch)]['pair']['cu_sn']['count'])\n",
    "        suming['cu_sn'] += curious_choices[str(_seed)][\"ep\"+str(epoch)]['pair']['cu_sn']['count']\n",
    "        \n",
    "        y[str(_seed)]['all'].append(curious_choices[str(_seed)][\"ep\"+str(epoch)]['pair']['all']['count'])\n",
    "        suming['all'] += curious_choices[str(_seed)][\"ep\"+str(epoch)]['pair']['all']['count']\n",
    "    \n",
    "    averaging['pl_sn'].append(suming['pl_sn']/len(uc.final_seeds))\n",
    "    averaging['pl_cu'].append(suming['pl_cu']/len(uc.final_seeds))\n",
    "    averaging['cu_sn'].append(suming['cu_sn']/len(uc.final_seeds))\n",
    "    averaging['all'].append(suming['all']/len(uc.final_seeds))\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n\\n\", \"-^*\"*10,\"\\n\", \"Total running time:\\t\", format(end_time-start_time, '.2f'), \"\\n\", \"-^*\"*10,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot\n",
    "Here we make the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7cd9a1978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=[10,10])\n",
    "figure.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "\n",
    "plt.ylim(0,1000)  # Limit the Y axis\n",
    "plt.plot(epochs, averaging['all'],\n",
    "         label='Overlap of all three', color=\"Green\", linewidth=1.5)\n",
    "plt.plot(epochs, averaging['pl_sn'],\n",
    "         label='Plasticity Vs. Subjective Novelty', color=\"red\", linewidth=1.5)\n",
    "plt.plot(epochs, averaging['pl_cu'],\n",
    "         label='Plasticity Vs. Curiosity', color=\"steelblue\", linewidth=1.5)\n",
    "plt.plot(epochs, averaging['cu_sn'],\n",
    "         label='Curiosity Vs. Subjective Novelty', color=\"darkorange\", linewidth=1.5)\n",
    "\n",
    "plt.legend(loc='upper center', fontsize=13)\n",
    "plt.ylabel('Overlap Number', fontsize=13)\n",
    "plt.xlabel('Epoch', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Savings\n",
    "\n",
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure.savefig('overlap_results/overlap_averaged_{}.svg'.format(setting), bbox_inches='tight', format='svg')\n",
    "figure.savefig('overlap_results/overlap_averaged_{}.pdf'.format(setting), bbox_inches='tight', format='pdf')\n",
    "figure.savefig('overlap_results/overlap_averaged_{}.png'.format(setting), bbox_inches='tight', format='png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averages in CSV Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Averages\n",
    "df_averages = pd.DataFrame.from_dict(averaging, orient='index')\n",
    "df_averages.to_csv('overlap_results/overlap_averages_{}.csv'.format(setting), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All results and averaes in Json\n",
    "\n",
    "The `y` dictionary has the shape:\n",
    "\n",
    "y == {`seed` : {`pair` : `[overlap numbers in order of epochs]`}}\n",
    "\n",
    "- `seed`: all the seeds of the experiment accessible by `uc.final_seeds`\n",
    "- `pair`:\n",
    "    - `pl_sn`: Plasticity <-> Subjective Novelty\n",
    "    - `pl_cu`: Plasticity <-> Curiosity\n",
    "    - `cn_sn`: Curiosity <-> Subjective Novelty\n",
    "    - `all`: overlap of all three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('overlap_results/all_overlap_{}.json'.format(setting), 'w') as fp:\n",
    "    json.dump(y, fp)\n",
    "\n",
    "with open('overlap_results/averages_{}.json'.format(setting), 'w') as fp:\n",
    "    json.dump(averaging, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
